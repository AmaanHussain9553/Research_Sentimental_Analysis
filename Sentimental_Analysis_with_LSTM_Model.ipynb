{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a507ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:05:39.681376Z",
     "iopub.status.busy": "2022-04-19T03:05:39.680369Z",
     "iopub.status.idle": "2022-04-19T03:05:39.688202Z",
     "shell.execute_reply": "2022-04-19T03:05:39.687573Z",
     "shell.execute_reply.started": "2022-04-19T03:05:39.681331Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries for dataframes and array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#text prepreprocessing library\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#library to map the words to numbers to pass into \n",
    "#the neural network\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#library to create the neural network and the various\n",
    "#layers in the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d4a89d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:05:39.709709Z",
     "iopub.status.busy": "2022-04-19T03:05:39.709011Z",
     "iopub.status.idle": "2022-04-19T03:05:40.720231Z",
     "shell.execute_reply": "2022-04-19T03:05:40.719391Z",
     "shell.execute_reply.started": "2022-04-19T03:05:39.709681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets: ['user father dysfunctional selfish drags kids dysfunction run', 'user user thanks lyft credit cant use cause offer wheelchair vans pdx disapointed getthanked', 'bihday majesty', 'model love u take u time urð ðððð ððð', 'factsguide society motivation']\n",
      "The length of tweets list is: 31962\n",
      "The length of labels list is: 31962\n"
     ]
    }
   ],
   "source": [
    "def load_as_list(fname):\n",
    "    \"\"\"Read in the dataset csv, store it in\n",
    "    a pandas dataframe and \"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    id = df['id'].values.tolist()\n",
    "    label = df['label'].values.tolist()\n",
    "    tweets = df['tweet'].values.tolist()\n",
    "    return tweets, label\n",
    "\n",
    "tweets, label = load_as_list(\"train.csv\")\n",
    "\n",
    "# removes stopwords and punctuation from tweets list\n",
    "for i in range(len(tweets)):\n",
    "    tweets[i] = ' '.join([word for word in tweets[i].split() if word not in cachedStopWords])\n",
    "    tweets[i] = re.sub(r'[^\\w\\s]', '', tweets[i])\n",
    "\n",
    "print(f'The first 5 tweets: {tweets[:5]}')\n",
    "print(f'The length of tweets list is: {len(tweets)}')\n",
    "print(f'The length of labels list is: {len(label)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac62bb21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:05:40.722550Z",
     "iopub.status.busy": "2022-04-19T03:05:40.721859Z",
     "iopub.status.idle": "2022-04-19T03:05:40.848304Z",
     "shell.execute_reply": "2022-04-19T03:05:40.847378Z",
     "shell.execute_reply.started": "2022-04-19T03:05:40.722513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n",
      "The number of unique vocab in twitter list is: 1304\n"
     ]
    }
   ],
   "source": [
    "# counts the number of unique vocab in the twitter tweets list\n",
    "word_count = {}\n",
    "for word in tweets:\n",
    "    word_list = word.split()\n",
    "    for sub_word in word_list:\n",
    "        if sub_word in word_count:\n",
    "            word_count[sub_word] += 1\n",
    "        else:\n",
    "            word_count[sub_word] = 1\n",
    "print(\"executed\")\n",
    "\n",
    "# filters only the vocab words that occur 30 or more times in the \n",
    "# twitter tweets list\n",
    "word_count = {key:val for key, val in word_count.items() if val >= 30}\n",
    "print(f'The number of unique vocab in twitter list is: {len(word_count)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd84bdd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:05:40.850504Z",
     "iopub.status.busy": "2022-04-19T03:05:40.850135Z",
     "iopub.status.idle": "2022-04-19T03:05:43.772786Z",
     "shell.execute_reply": "2022-04-19T03:05:43.771759Z",
     "shell.execute_reply.started": "2022-04-19T03:05:40.850467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets: ['user father dysfunctional selfish drags kids dysfunction run', 'user user thanks lyft credit cant use cause offer wheelchair vans pdx disapointed getthanked', 'bihday majesty', 'model love u take u time urð ðððð ððð', 'factsguide society motivation']\n",
      "The length of tweets list is: 31962\n",
      "The length of labels list is: 31962\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "# removing the uncommon words and only keeping the vocab words \n",
    "# with a frequency of 30 or more in the tweets list\n",
    "accepted_list = list(word_count.keys())\n",
    "\n",
    "for word in tweets:\n",
    "    word_list = word.split()\n",
    "    final_word_list = [word for word in word_list if word in accepted_list]\n",
    "    word = ' '.join(final_word_list)\n",
    "  \n",
    "print(f'The first 5 tweets: {tweets[:5]}')\n",
    "print(f'The length of tweets list is: {len(tweets)}')\n",
    "print(f'The length of labels list is: {len(label)}')\n",
    "\n",
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    print(max(list_len))\n",
    "\n",
    "#print output#\n",
    "find_max_list(tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46d5de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:05:43.774044Z",
     "iopub.status.busy": "2022-04-19T03:05:43.773840Z",
     "iopub.status.idle": "2022-04-19T03:05:44.581537Z",
     "shell.execute_reply": "2022-04-19T03:05:44.580669Z",
     "shell.execute_reply.started": "2022-04-19T03:05:43.774018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets after encoding is: [[1245  569  449  370  829  568  315  568    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1245 1245 1166  707  723  555  570 1068   24  995 1241 1166  845 1208\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 175  657    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 165 1033  142  376  142 1163  305  141  874    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 734 1291 1019    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "25570\n",
      "The first 5 labels are: [[1245  869  968 1202  404  584  144  615  515  772    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 538 1287  377  287 1033 1091  124 1282 1124  883 1223    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1125  814 1245  972  673  735   24 1163   46    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 153  752  990   52   28  395 1140  814  140  424 1031 1017  384  931\n",
      "   299 1130    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1107  912  250  250  206  982  834   24  407  538  192    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "6392\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1304\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in tweets]\n",
    "\n",
    "\n",
    "# pad documents to a max length of 128 words\n",
    "max_length = 128\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "x_train = padded_docs[:25570]\n",
    "y_train = label[:25570]\n",
    "\n",
    "x_test = padded_docs[25570:]\n",
    "y_test = label[25570:]\n",
    "\n",
    "print(f'The first 5 tweets after encoding is: {x_train[:5]}')\n",
    "print(len(x_train))\n",
    "print(f'The first 5 labels are: {x_test[:5]}')\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b653a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:34:44.612175Z",
     "iopub.status.busy": "2022-04-19T03:34:44.611900Z",
     "iopub.status.idle": "2022-04-19T03:49:37.929256Z",
     "shell.execute_reply": "2022-04-19T03:49:37.928432Z",
     "shell.execute_reply.started": "2022-04-19T03:34:44.612138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1599/1599 [==============================] - 179s 111ms/step - loss: 0.1981 - accuracy: 0.9373\n",
      "Epoch 2/5\n",
      "1599/1599 [==============================] - 178s 111ms/step - loss: 0.1452 - accuracy: 0.9496\n",
      "Epoch 3/5\n",
      "1599/1599 [==============================] - 178s 112ms/step - loss: 0.1176 - accuracy: 0.9576\n",
      "Epoch 4/5\n",
      "1599/1599 [==============================] - 178s 111ms/step - loss: 0.0877 - accuracy: 0.9683\n",
      "Epoch 5/5\n",
      "1599/1599 [==============================] - 178s 111ms/step - loss: 0.0595 - accuracy: 0.9780\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 256, input_length=max_length))\n",
    "model.add(LSTM(1024, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(x_train, np.array(y_train), epochs= 5, batch_size = 16)\n",
    "\n",
    "with open('model_summary_RNN.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1255aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:49:37.931357Z",
     "iopub.status.busy": "2022-04-19T03:49:37.931071Z",
     "iopub.status.idle": "2022-04-19T03:50:25.827681Z",
     "shell.execute_reply": "2022-04-19T03:50:25.826217Z",
     "shell.execute_reply.started": "2022-04-19T03:49:37.931321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 48s 59ms/step - loss: 0.0309 - accuracy: 0.9911\n",
      "Accuracy: 99.112242\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(np.array(x_train), np.array(y_train))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d95209-3d6b-42a1-a65d-98685e96a34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T03:50:25.828979Z",
     "iopub.status.busy": "2022-04-19T03:50:25.828802Z",
     "iopub.status.idle": "2022-04-19T03:50:37.476727Z",
     "shell.execute_reply": "2022-04-19T03:50:37.475908Z",
     "shell.execute_reply.started": "2022-04-19T03:50:25.828955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "[1, 0, 0, 0, 0]\n",
      "Confusion Matrix Matrics (Test dataset size: 6392)\n",
      "True Negatives: 5740\n",
      "False Negatives: 240\n",
      "True Positives: 203\n",
      "False Positives: 209\n"
     ]
    }
   ],
   "source": [
    "#predict the model \n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred[:5])\n",
    "print(y_test[:5])\n",
    "\n",
    "#Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(f'Confusion Matrix Matrics (Test dataset size: {len(y_test)})')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Negatives: {fn}')\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
