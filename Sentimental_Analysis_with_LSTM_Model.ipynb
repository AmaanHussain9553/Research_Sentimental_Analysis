{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6a507ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:10:37.796562Z",
     "iopub.status.busy": "2022-04-18T03:10:37.796311Z",
     "iopub.status.idle": "2022-04-18T03:10:37.802856Z",
     "shell.execute_reply": "2022-04-18T03:10:37.802236Z",
     "shell.execute_reply.started": "2022-04-18T03:10:37.796537Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries for dataframes and array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#text prepreprocessing library\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#library to map the words to numbers to pass into \n",
    "#the neural network\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#library to create the neural network and the various\n",
    "#layers in the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9d4a89d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:10:37.814149Z",
     "iopub.status.busy": "2022-04-18T03:10:37.813990Z",
     "iopub.status.idle": "2022-04-18T03:10:38.708727Z",
     "shell.execute_reply": "2022-04-18T03:10:38.708047Z",
     "shell.execute_reply.started": "2022-04-18T03:10:37.814130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets: ['user father dysfunctional selfish drags kids dysfunction run', 'user user thanks lyft credit cant use cause offer wheelchair vans pdx disapointed getthanked', 'bihday majesty', 'model love u take u time urð ðððð ððð', 'factsguide society motivation']\n",
      "The length of tweets list is: 31962\n",
      "The length of labels list is: 31962\n"
     ]
    }
   ],
   "source": [
    "def load_as_list(fname):\n",
    "    \"\"\"Read in the dataset csv, store it in\n",
    "    a pandas dataframe and \"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    id = df['id'].values.tolist()\n",
    "    label = df['label'].values.tolist()\n",
    "    tweets = df['tweet'].values.tolist()\n",
    "    return tweets, label\n",
    "\n",
    "tweets, label = load_as_list(\"train.csv\")\n",
    "\n",
    "# removes stopwords and punctuation from tweets list\n",
    "for i in range(len(tweets)):\n",
    "    tweets[i] = ' '.join([word for word in tweets[i].split() if word not in cachedStopWords])\n",
    "    tweets[i] = re.sub(r'[^\\w\\s]', '', tweets[i])\n",
    "\n",
    "print(f'The first 5 tweets: {tweets[:5]}')\n",
    "print(f'The length of tweets list is: {len(tweets)}')\n",
    "print(f'The length of labels list is: {len(label)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac62bb21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:10:38.710623Z",
     "iopub.status.busy": "2022-04-18T03:10:38.710180Z",
     "iopub.status.idle": "2022-04-18T03:10:38.814534Z",
     "shell.execute_reply": "2022-04-18T03:10:38.814015Z",
     "shell.execute_reply.started": "2022-04-18T03:10:38.710583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n",
      "The number of unique vocab in twitter list is: 1304\n"
     ]
    }
   ],
   "source": [
    "# counts the number of unique vocab in the twitter tweets list\n",
    "word_count = {}\n",
    "for word in tweets:\n",
    "    word_list = word.split()\n",
    "    for sub_word in word_list:\n",
    "        if sub_word in word_count:\n",
    "            word_count[sub_word] += 1\n",
    "        else:\n",
    "            word_count[sub_word] = 1\n",
    "print(\"executed\")\n",
    "\n",
    "# filters only the vocab words that occur 30 or more times in the \n",
    "# twitter tweets list\n",
    "word_count = {key:val for key, val in word_count.items() if val >= 30}\n",
    "print(f'The number of unique vocab in twitter list is: {len(word_count)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd84bdd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:10:38.816295Z",
     "iopub.status.busy": "2022-04-18T03:10:38.815424Z",
     "iopub.status.idle": "2022-04-18T03:10:41.384564Z",
     "shell.execute_reply": "2022-04-18T03:10:41.384017Z",
     "shell.execute_reply.started": "2022-04-18T03:10:38.816268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets: ['user father dysfunctional selfish drags kids dysfunction run', 'user user thanks lyft credit cant use cause offer wheelchair vans pdx disapointed getthanked', 'bihday majesty', 'model love u take u time urð ðððð ððð', 'factsguide society motivation']\n",
      "The length of tweets list is: 31962\n",
      "The length of labels list is: 31962\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "# removing the uncommon words and only keeping the vocab words \n",
    "# with a frequency of 30 or more in the tweets list\n",
    "accepted_list = list(word_count.keys())\n",
    "\n",
    "for word in tweets:\n",
    "    word_list = word.split()\n",
    "    final_word_list = [word for word in word_list if word in accepted_list]\n",
    "    word = ' '.join(final_word_list)\n",
    "  \n",
    "print(f'The first 5 tweets: {tweets[:5]}')\n",
    "print(f'The length of tweets list is: {len(tweets)}')\n",
    "print(f'The length of labels list is: {len(label)}')\n",
    "\n",
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    print(max(list_len))\n",
    "\n",
    "#print output#\n",
    "find_max_list(tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e46d5de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:10:41.386412Z",
     "iopub.status.busy": "2022-04-18T03:10:41.386220Z",
     "iopub.status.idle": "2022-04-18T03:10:41.925243Z",
     "shell.execute_reply": "2022-04-18T03:10:41.924340Z",
     "shell.execute_reply.started": "2022-04-18T03:10:41.386389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets after encoding is: [[ 726  174 1266  787  267  272 1106  811    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 726  726   76  243  802  763  149   10  348  376 1150 1216  446  110\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  90 1226    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 393 1195  889 1271  889 1204 1077  241  679    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 559  422 1259    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "25570\n",
      "The first 5 labels are: [[ 726   20  534 1185  208  367 1297  396 1241  556    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 703  120  904 1027 1195 1235  399   98  350   56  744    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 284  555  726 1213  604  544  609 1204  237    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1000   72  801  907 1067  505  460  555  980 1224 1289 1239  449  638\n",
      "   222  178    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 251 1194 1278 1278  101  720  504  609  149  703  620    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "6392\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1304\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in tweets]\n",
    "\n",
    "\n",
    "# pad documents to a max length of 128 words\n",
    "max_length = 128\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "x_train = padded_docs[:25570]\n",
    "y_train = label[:25570]\n",
    "\n",
    "x_test = padded_docs[25570:]\n",
    "y_test = label[25570:]\n",
    "\n",
    "print(f'The first 5 tweets after encoding is: {x_train[:5]}')\n",
    "print(len(x_train))\n",
    "print(f'The first 5 labels are: {x_test[:5]}')\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b653a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T03:23:43.777479Z",
     "iopub.status.busy": "2022-04-18T03:23:43.776727Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 128, 256)          333824    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128, 1024)         5246976   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 131073    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,711,873\n",
      "Trainable params: 5,711,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1599/1599 [==============================] - 180s 112ms/step - loss: 0.1945 - accuracy: 0.9396\n",
      "Epoch 2/5\n",
      "1599/1599 [==============================] - 178s 112ms/step - loss: 0.1433 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "1599/1599 [==============================] - 178s 112ms/step - loss: 0.1118 - accuracy: 0.9607\n",
      "Epoch 4/5\n",
      "1599/1599 [==============================] - 178s 112ms/step - loss: 0.0794 - accuracy: 0.9723\n",
      "Epoch 5/5\n",
      " 806/1599 [==============>...............] - ETA: 1:28 - loss: 0.0433 - accuracy: 0.9854"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 256, input_length=max_length))\n",
    "model.add(LSTM(1024, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(x_train, np.array(y_train), epochs= 5, batch_size = 16)\n",
    "\n",
    "# evaluate the model\n",
    "# loss, accuracy = model.evaluate(np.array(x_train), np.array(y_train))\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1255aa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-18T03:23:42.248771Z",
     "iopub.status.idle": "2022-04-18T03:23:42.249631Z",
     "shell.execute_reply": "2022-04-18T03:23:42.249459Z",
     "shell.execute_reply.started": "2022-04-18T03:23:42.249439Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(np.array(x_train), np.array(y_train))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d95209-3d6b-42a1-a65d-98685e96a34c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-18T03:23:42.250708Z",
     "iopub.status.idle": "2022-04-18T03:23:42.251314Z",
     "shell.execute_reply": "2022-04-18T03:23:42.251170Z",
     "shell.execute_reply.started": "2022-04-18T03:23:42.251147Z"
    }
   },
   "outputs": [],
   "source": [
    "#predict the model \n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred[:5])\n",
    "print(y_test[:5])\n",
    "\n",
    "#Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
