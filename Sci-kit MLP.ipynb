{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed90b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EMBEDDING_FILE = \"w2v.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b783a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the word embedding file \"w2v.pkl\"\n",
    "def load_w2v(filepath):\n",
    "    with open(filepath, 'rb') as fin:\n",
    "        return pkl.load(fin)\n",
    "\n",
    "# returns the embedding as a list\n",
    "def w2v(word2vec, token):\n",
    "    word_vector = np.zeros(300, )\n",
    "\n",
    "    # [YOUR CODE HERE]\n",
    "    word2vec = load_w2v(EMBEDDING_FILE)\n",
    "    if token in word2vec:\n",
    "        word_vector = word2vec[token]\n",
    "\n",
    "    return np.array(word_vector)\n",
    "\n",
    "# can read in data file and return list based on separator\n",
    "# reads in the train and test data and returns it as a pandas list\n",
    "def load_as_list(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    id = df['id'].values.tolist()\n",
    "    label = df['label'].values.tolist()\n",
    "    tweets = df['tweet'].values.tolist()\n",
    "    return tweets, label\n",
    "\n",
    "#tokenizes string\n",
    "def get_tokens(inp_str):\n",
    "    return inp_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71227e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2vec(word2vec, user_input):\n",
    "    embedding = np.zeros(300, )\n",
    "\n",
    "    # Tokenizing the string\n",
    "    tokenized_input = get_tokens(user_input)\n",
    "\n",
    "    # storing all the word vector of each of the\n",
    "    # tokenized string in a list of arrays\n",
    "    all_word_vectors = np.zeros(shape=(len(tokenized_input), 300))\n",
    "    for i in range(len(tokenized_input)):\n",
    "        all_word_vectors[i] = w2v(word2vec, tokenized_input[i])\n",
    "\n",
    "    # Adding up all the word embeddings array\n",
    "    embedding_sum = np.sum(all_word_vectors, axis=0)\n",
    "\n",
    "    # Dividing word embeddings array\n",
    "    # After this step we should have the embeddings array\n",
    "    embedding = embedding_sum / len(tokenized_input)\n",
    "    embedding = np.array(embedding)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bda8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_train(training_documents):\n",
    "    # Initialize the TfidfVectorizer model and document-term matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_train = None\n",
    "    # [YOUR CODE HERE]\n",
    "    tfidf_train = vectorizer.fit_transform(training_documents)\n",
    "\n",
    "    return vectorizer, tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455cfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, word2vec, training_documents, training_labels):\n",
    "    # Write your code here\n",
    "    # print(training_documents)\n",
    "    training_documents_array = np.array(training_documents)\n",
    "    training_documents_trained = []\n",
    "    for i in range(len(training_documents_array)):\n",
    "        training_documents_trained.append(string2vec(word2vec, training_documents[i]))\n",
    "    model.fit(training_documents_trained, training_labels)\n",
    "    return model\n",
    "\n",
    "def test_model(model, word2vec, test_documents, test_labels):\n",
    "    training_documents_array = np.array(test_documents)\n",
    "    training_documents_trained = []\n",
    "    for i in range(len(training_documents_array)):\n",
    "        training_documents_trained.append(string2vec(word2vec, test_documents[i]))\n",
    "\n",
    "    pred = model.predict(training_documents_trained)\n",
    "    print(pred)\n",
    "    precision = precision_score(test_labels, pred)\n",
    "    recall = recall_score(test_labels, pred)\n",
    "    f1 = f1_score(test_labels, pred)\n",
    "    accuracy = accuracy_score(test_labels, pred)\n",
    "\n",
    "    return round(precision, 2), round(recall, 2), round(f1, 2), round(accuracy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb0ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx____TFIDF START____xxxx \n",
      "\n",
      "  (0, 31398)\t0.24587523640473063\n",
      "  (0, 11441)\t0.37328100670717695\n",
      "  (0, 18940)\t0.2205095089178642\n",
      "  (0, 20271)\t0.2217974158889632\n",
      "  (0, 17200)\t0.379914618721787\n",
      "  (0, 11118)\t0.35472088764116205\n",
      "  (0, 16758)\t0.18302237090130583\n",
      "  (0, 32276)\t0.3200037447188773\n",
      "  (0, 33622)\t0.13945209452277157\n",
      "  (0, 2405)\t0.10773554811927136\n",
      "  (0, 11442)\t0.388013001997706\n",
      "  (0, 19101)\t0.2263921129123013\n",
      "  (0, 13066)\t0.18433419465066356\n",
      "  (0, 39790)\t0.15463350244422686\n",
      "  (0, 38416)\t0.07466312011486949\n",
      "  (1, 15090)\t0.3279202126540206\n",
      "  (1, 10522)\t0.34086198826458086\n",
      "  (1, 27504)\t0.31873787672819126\n",
      "  (1, 18369)\t0.09644664542534936\n",
      "  (1, 38561)\t0.3279202126540206\n",
      "  (1, 39784)\t0.31873787672819126\n",
      "  (1, 26284)\t0.257788438360121\n",
      "  (1, 10904)\t0.15684662390347334\n",
      "  (1, 36533)\t0.15263459071612748\n",
      "  (1, 6907)\t0.22604173086962553\n",
      "  :\t:\n",
      "  (31958, 18369)\t0.08516260158505909\n",
      "  (31959, 26830)\t0.4502363745857929\n",
      "  (31959, 21561)\t0.34994315261917547\n",
      "  (31959, 33828)\t0.36609346975257884\n",
      "  (31959, 40378)\t0.23605057914729688\n",
      "  (31959, 24175)\t0.24481483718166266\n",
      "  (31959, 24028)\t0.28870896495794574\n",
      "  (31959, 26452)\t0.16323267530493182\n",
      "  (31959, 31533)\t0.49630011135429186\n",
      "  (31959, 36938)\t0.21953117676845377\n",
      "  (31959, 19101)\t0.1436762926906704\n",
      "  (31960, 1410)\t0.3249899152294539\n",
      "  (31960, 8379)\t0.3706335215514262\n",
      "  (31960, 40569)\t0.3706335215514262\n",
      "  (31960, 6430)\t0.36283679046592054\n",
      "  (31960, 38541)\t0.3706335215514262\n",
      "  (31960, 32999)\t0.36896459659951414\n",
      "  (31960, 36034)\t0.35872445459590296\n",
      "  (31960, 18369)\t0.27733097778328314\n",
      "  (31960, 38416)\t0.09430173637587742\n",
      "  (31961, 13939)\t0.5399796690215206\n",
      "  (31961, 36187)\t0.529164824742413\n",
      "  (31961, 40949)\t0.5585176657142643\n",
      "  (31961, 14033)\t0.2820980839625356\n",
      "  (31961, 38416)\t0.19205528737448452\n",
      "xxxx_____TFIDF END_____xxxx\n"
     ]
    }
   ],
   "source": [
    "tweets, label = load_as_list(\"train.csv\")\n",
    "vectorizer, tfidf_train = vectorize_train(tweets)\n",
    "\n",
    "# testing begins here\n",
    "test_data, test_labels = load_as_list(\"train.csv\")\n",
    "\n",
    "print(\"xxxx____TFIDF START____xxxx \\n\")\n",
    "print(tfidf_train)\n",
    "print(\"xxxx_____TFIDF END_____xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf8047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec\n"
     ]
    }
   ],
   "source": [
    "word2vec = load_w2v(EMBEDDING_FILE)\n",
    "print(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLPClassifier()\n",
    "# mlp.fit()\n",
    "i = 0\n",
    "df = pd.DataFrame(columns = ['alpha','max_iter','train_acc','test_acc','train_time'])\n",
    "for a in [0.00001,0.0001,0.001,0.01, 0.1, 1, 10]:\n",
    "    for mi in [10,100,200,500,1000,2000]:\n",
    "        st = time()\n",
    "        mlp = MLPClassifier(alpha=a, max_iter=mi)\n",
    "        mlp.fit(tfidf_train, trlab)\n",
    "        end = time() - st\n",
    "        \n",
    "        acc_tr = accuracy_score(trlab, mlp.predict(train)) # Train Accuracy\n",
    "        acc = accuracy_score(tslab, mlp.predict(test)) # Test Accuracy\n",
    "        df.loc[i] = [a,mi,acc_tr,acc,end]\n",
    "        i=i+1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "acc_tr = []\n",
    "timelog = []\n",
    "for l in [10,20,50,100,200,500,1000]:\n",
    "    t = time()\n",
    "    mlp = MLPClassifier(alpha=0.1, max_iter=200, hidden_layer_sizes=(l,))\n",
    "    mlp.fit(train, trlab)\n",
    "    endt = time() - t\n",
    "        \n",
    "    a_tr = accuracy_score(trlab, mlp.predict(train)) # Train Accuracy\n",
    "    a = accuracy_score(tslab, mlp.predict(test)) # Test Accuracy\n",
    "\n",
    "    acc_tr.append(a_tr)\n",
    "    acc.append(a)\n",
    "    timelog.append(endt)\n",
    "    \n",
    "l = [10,20,50,100,200,500,1000]\n",
    "N = len(l)\n",
    "l2 = np.arange(N)\n",
    "matplot.subplots(figsize=(10, 5))\n",
    "matplot.plot(l2, acc, label=\"Testing Accuracy\")\n",
    "matplot.plot(l2, acc_tr, label=\"Training Accuracy\")\n",
    "matplot.xticks(l2,l)\n",
    "matplot.grid(True)\n",
    "matplot.xlabel(\"Hidden Layer Nodes\")\n",
    "matplot.ylabel(\"Accuracy\")\n",
    "matplot.legend()\n",
    "matplot.title('Accuracy versus Nodes in the Hidden Layer for MLPClassifier', fontsize=12)\n",
    "matplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d665ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [10,20,50,100,200,500,1000]\n",
    "N = len(l)\n",
    "l2 = np.arange(N)\n",
    "matplot.subplots(figsize=(10, 5))\n",
    "matplot.plot(l2, timelog, label=\"Training time in s\")\n",
    "matplot.xticks(l2,l)\n",
    "matplot.grid(True)\n",
    "matplot.xlabel(\"Hidden Layer Nodes\")\n",
    "matplot.ylabel(\"Time (s)\")\n",
    "matplot.legend()\n",
    "matplot.title('Training Time versus Nodes in the Hidden Layer for MLPClassifier', fontsize=12)\n",
    "matplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"classification_report.csv\", \"w\")\n",
    "outfile_writer = csv.writer(outfile)\n",
    "outfile_writer.writerow([\"Name\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"])  # Header row\n",
    "p, r, f, a = test_model(mlp, word2vec, test_data, test_labels)\n",
    "outfile_writer.writerow([mlp, p, r, f, a])\n",
    "outfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test = string2vec(word2vec, test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45941e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = mlp.predict(w2v_test.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad659b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if label == 0:\n",
    "    print(\"Great!  It sounds like you are a decent human being.\")\n",
    "elif label == 1:\n",
    "    print(\"Oh no!  It sounds like you're being offensive.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
