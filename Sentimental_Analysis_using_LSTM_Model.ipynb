{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a507ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T22:53:59.591751Z",
     "iopub.status.busy": "2022-04-23T22:53:59.591447Z",
     "iopub.status.idle": "2022-04-23T22:54:02.654271Z",
     "shell.execute_reply": "2022-04-23T22:54:02.653481Z",
     "shell.execute_reply.started": "2022-04-23T22:53:59.591677Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# libraries for dataframes and array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#text prepreprocessing library\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#library to map the words to numbers to pass into \n",
    "#the neural network\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#library to create the neural network and the various\n",
    "#layers in the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d4a89d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T22:54:02.655932Z",
     "iopub.status.busy": "2022-04-23T22:54:02.655744Z",
     "iopub.status.idle": "2022-04-23T22:54:03.672400Z",
     "shell.execute_reply": "2022-04-23T22:54:03.671722Z",
     "shell.execute_reply.started": "2022-04-23T22:54:02.655908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets: ['user father dysfunctional selfish drags kids dysfunction run', 'user user thanks lyft credit cant use cause offer wheelchair vans pdx disapointed getthanked', 'bihday majesty', 'model love u take u time urð ðððð ððð', 'factsguide society motivation']\n",
      "The length of tweets list is: 31962\n",
      "The length of labels list is: 31962\n"
     ]
    }
   ],
   "source": [
    "def load_as_list(fname):\n",
    "    \"\"\"Read in the dataset csv, store it in\n",
    "    a pandas dataframe and \"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    id = df['id'].values.tolist()\n",
    "    label = df['label'].values.tolist()\n",
    "    tweets = df['tweet'].values.tolist()\n",
    "    return tweets, label\n",
    "\n",
    "tweets, label = load_as_list(\"train.csv\")\n",
    "\n",
    "# removes stopwords and punctuation from tweets list\n",
    "for i in range(len(tweets)):\n",
    "    tweets[i] = ' '.join([word for word in tweets[i].split() if word not in cachedStopWords])\n",
    "    tweets[i] = re.sub(r'[^\\w\\s]', '', tweets[i])\n",
    "\n",
    "print(f'The first 5 tweets: {tweets[:5]}')\n",
    "print(f'The length of tweets list is: {len(tweets)}')\n",
    "print(f'The length of labels list is: {len(label)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac62bb21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T22:54:03.673692Z",
     "iopub.status.busy": "2022-04-23T22:54:03.673493Z",
     "iopub.status.idle": "2022-04-23T22:54:03.794403Z",
     "shell.execute_reply": "2022-04-23T22:54:03.793624Z",
     "shell.execute_reply.started": "2022-04-23T22:54:03.673667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n",
      "The number of unique vocab in twitter list is: 1304\n"
     ]
    }
   ],
   "source": [
    "# counts the number of unique vocab in the twitter tweets list\n",
    "word_count = {}\n",
    "for word in tweets:\n",
    "    word_list = word.split()\n",
    "    for sub_word in word_list:\n",
    "        if sub_word in word_count:\n",
    "            word_count[sub_word] += 1\n",
    "        else:\n",
    "            word_count[sub_word] = 1\n",
    "print(\"executed\")\n",
    "\n",
    "# filters only the vocab words that occur 30 or more times in the \n",
    "# twitter tweets list\n",
    "word_count = {key:val for key, val in word_count.items() if val >= 30}\n",
    "print(f'The number of unique vocab in twitter list is: {len(word_count)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd84bdd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T22:54:03.795755Z",
     "iopub.status.busy": "2022-04-23T22:54:03.795512Z",
     "iopub.status.idle": "2022-04-23T22:54:06.626052Z",
     "shell.execute_reply": "2022-04-23T22:54:06.625296Z",
     "shell.execute_reply.started": "2022-04-23T22:54:03.795718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets: ['user father dysfunctional selfish drags kids dysfunction run', 'user user thanks lyft credit cant use cause offer wheelchair vans pdx disapointed getthanked', 'bihday majesty', 'model love u take u time urð ðððð ððð', 'factsguide society motivation']\n",
      "The length of tweets list is: 31962\n",
      "The length of labels list is: 31962\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "# removing the uncommon words and only keeping the vocab words \n",
    "# with a frequency of 30 or more in the tweets list\n",
    "accepted_list = list(word_count.keys())\n",
    "\n",
    "for word in tweets:\n",
    "    word_list = word.split()\n",
    "    final_word_list = [word for word in word_list if word in accepted_list]\n",
    "    word = ' '.join(final_word_list)\n",
    "  \n",
    "print(f'The first 5 tweets: {tweets[:5]}')\n",
    "print(f'The length of tweets list is: {len(tweets)}')\n",
    "print(f'The length of labels list is: {len(label)}')\n",
    "\n",
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    print(max(list_len))\n",
    "\n",
    "#print output#\n",
    "find_max_list(tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46d5de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T23:23:25.683846Z",
     "iopub.status.busy": "2022-04-23T23:23:25.683187Z",
     "iopub.status.idle": "2022-04-23T23:23:26.544151Z",
     "shell.execute_reply": "2022-04-23T23:23:26.543333Z",
     "shell.execute_reply.started": "2022-04-23T23:23:25.683818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 tweets after encoding is:\n",
      " [[ 106  819  722  415  378  728  536  946    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 106  106  509  809  471  617  736 1040  283  441  779  911   69  608\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1254   36    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 217  543  791  400  791  395  810  893  968    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [ 510  625  179    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "The number of tweets in test data is:\n",
      " 25570\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1304\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in tweets]\n",
    "\n",
    "\n",
    "# pad documents to a max length of 128 words\n",
    "max_length = 128\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "x_train = padded_docs[:25570]\n",
    "y_train = label[:25570]\n",
    "\n",
    "x_test = padded_docs[25570:]\n",
    "y_test = label[25570:]\n",
    "\n",
    "print(f'The first 5 tweets after encoding is:\\n {x_train[:5]}')\n",
    "print(f'The number of tweets in test data is:\\n {len(x_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b653a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T23:23:32.613559Z",
     "iopub.status.busy": "2022-04-23T23:23:32.613279Z",
     "iopub.status.idle": "2022-04-23T23:42:36.920470Z",
     "shell.execute_reply": "2022-04-23T23:42:36.919636Z",
     "shell.execute_reply.started": "2022-04-23T23:23:32.613533Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_output: 512, Batch_Size: 8\n",
      "Epoch 1/5\n",
      "3197/3197 [==============================] - 230s 71ms/step - loss: 0.2115 - accuracy: 0.9363\n",
      "Epoch 2/5\n",
      "3197/3197 [==============================] - 228s 71ms/step - loss: 0.1487 - accuracy: 0.9496\n",
      "Epoch 3/5\n",
      "3197/3197 [==============================] - 228s 71ms/step - loss: 0.1128 - accuracy: 0.9617\n",
      "Epoch 4/5\n",
      "3197/3197 [==============================] - 228s 71ms/step - loss: 0.0696 - accuracy: 0.9751\n",
      "Epoch 5/5\n",
      "3197/3197 [==============================] - 228s 71ms/step - loss: 0.0411 - accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Embedding_output: 512, Batch_Size: 8')\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 512, input_length=max_length))\n",
    "model.add(LSTM(1024, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(x_train, np.array(y_train), epochs= 5, batch_size = 8)\n",
    "\n",
    "with open('model_summary_RNN.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1255aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T23:13:11.454537Z",
     "iopub.status.busy": "2022-04-23T23:13:11.454354Z",
     "iopub.status.idle": "2022-04-23T23:14:03.802677Z",
     "shell.execute_reply": "2022-04-23T23:14:03.801903Z",
     "shell.execute_reply.started": "2022-04-23T23:13:11.454514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 52s 65ms/step - loss: 0.0204 - accuracy: 0.9939\n",
      "Accuracy: 99.393821\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(np.array(x_train), np.array(y_train))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d95209-3d6b-42a1-a65d-98685e96a34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T23:45:31.957140Z",
     "iopub.status.busy": "2022-04-23T23:45:31.956889Z",
     "iopub.status.idle": "2022-04-23T23:45:44.723463Z",
     "shell.execute_reply": "2022-04-23T23:45:44.722613Z",
     "shell.execute_reply.started": "2022-04-23T23:45:31.957114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Matrics (Test dataset size: 6392)\n",
      "True Negatives: 5792\n",
      "False Negatives: 249\n",
      "True Positives: 194\n",
      "False Positives: 157\n"
     ]
    }
   ],
   "source": [
    "#predict the model \n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "#Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(f'Confusion Matrix Matrics (Test dataset size: {len(y_test)})')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Negatives: {fn}')\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
